{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "LogisticRegression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDRdayHsN9bW"
      },
      "source": [
        "import pandas as pd # pd is simply an alias \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9FYwQEpN9ba"
      },
      "source": [
        "## Part I. Pre-processing a real dataset\n",
        "\n",
        "The [Breast Cancer Wisconsin (Diagnostic) Data Set](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)) contains 10 features related to breast tumors that have been diagnosed as benign or malignant. You have a description of the 10 features in the link above. In the same link you can download the data set as a CVS file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YBrg0OhN9bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "outputId": "51af9a87-cb01-439a-8fa7-d4b5c56a52ab"
      },
      "source": [
        "''' Data is stored in a Dataframe, a particular data type implemented in Pandas'''\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "data = pd.read_csv('./breast-cancer-wisconsin.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ee8b38fa-e575-4e77-ab04-2d067dd43d60\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-ee8b38fa-e575-4e77-ab04-2d067dd43d60\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-e1a50bc84c5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./breast-cancer-wisconsin.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m   result = _output.eval_js(\n\u001b[1;32m     63\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[0;32m---> 64\u001b[0;31m           input_id=input_id, output_id=output_id))\n\u001b[0m\u001b[1;32m     65\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: TypeError: Cannot read property '_uploadFiles' of undefined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFvqF_zEN9be"
      },
      "source": [
        "Lets visualize the first 10 entries of the database with the `.head()` method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7DVWHV7N9bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "ac6cd162-0b99-4488-d219-d02e4a61f41c"
      },
      "source": [
        "data.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Clump Thickness</th>\n",
              "      <th>Uniformity of Cell Size</th>\n",
              "      <th>Uniformity of Cell Shape</th>\n",
              "      <th>Marginal Adhesion</th>\n",
              "      <th>Single Epithelial Cell Size</th>\n",
              "      <th>Bare Nuclei</th>\n",
              "      <th>Bland Chromatin</th>\n",
              "      <th>Normal Nucleoli</th>\n",
              "      <th>Mitoses</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1000025</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1002945</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>10.0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1015425</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1016277</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1017023</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1017122</td>\n",
              "      <td>8</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>10.0</td>\n",
              "      <td>9</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1018099</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>10.0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1018561</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1033078</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1033078</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        ID  Clump Thickness  ...  Mitoses  Class\n",
              "0  1000025                5  ...        1      2\n",
              "1  1002945                5  ...        1      2\n",
              "2  1015425                3  ...        1      2\n",
              "3  1016277                6  ...        1      2\n",
              "4  1017023                4  ...        1      2\n",
              "5  1017122                8  ...        1      4\n",
              "6  1018099                1  ...        1      2\n",
              "7  1018561                2  ...        1      2\n",
              "8  1033078                2  ...        5      2\n",
              "9  1033078                4  ...        1      2\n",
              "\n",
              "[10 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KW8AlWwRN9bj"
      },
      "source": [
        "With the `.shape()` method we can check how many datapoints we have"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVYhCH2ON9bk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f86fc839-aeb7-4f59-91ca-20a55cbd35d3"
      },
      "source": [
        "print('There are %d data points. Each one of dimension %d' %(data.shape[0],data.shape[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 699 data points. Each one of dimension 11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ml5u57QHN9bn"
      },
      "source": [
        "We will do the following preprocessing steps. All of them are implemented in Pandas (it is advisory for very large databases):\n",
        "\n",
        "- Remove the 'ID' column. We won't use it anymore\n",
        "\n",
        "- Replace missing data (Encoded in this database with a '?' by the column mode). \n",
        "\n",
        "- The tumor class is equal to 2 for bening tumors and to 4 for malignat tumors. We will replace this by 0 and 1 respectively\n",
        "\n",
        "- Create a train set and a test set\n",
        "\n",
        "- We will normalize all input variables so they all have zero mean and unit variance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRB04l1NN9bo"
      },
      "source": [
        "### Remove ID colum\n",
        "\n",
        "We can use the `.drop()` method. We use the input `inplace=True` to override the existing Dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbogxjDgN9bp"
      },
      "source": [
        "data.drop('ID',axis=1,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfsE6x3FN9bs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "73df898c-a510-44ec-a9f5-9c301363ae85"
      },
      "source": [
        "# Lets check the result\n",
        "\n",
        "data.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Clump Thickness</th>\n",
              "      <th>Uniformity of Cell Size</th>\n",
              "      <th>Uniformity of Cell Shape</th>\n",
              "      <th>Marginal Adhesion</th>\n",
              "      <th>Single Epithelial Cell Size</th>\n",
              "      <th>Bare Nuclei</th>\n",
              "      <th>Bland Chromatin</th>\n",
              "      <th>Normal Nucleoli</th>\n",
              "      <th>Mitoses</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>10.0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>8</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>10.0</td>\n",
              "      <td>9</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>10.0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Clump Thickness  Uniformity of Cell Size  ...  Mitoses  Class\n",
              "0                5                        1  ...        1      2\n",
              "1                5                        4  ...        1      2\n",
              "2                3                        1  ...        1      2\n",
              "3                6                        8  ...        1      2\n",
              "4                4                        1  ...        1      2\n",
              "5                8                       10  ...        1      4\n",
              "6                1                        1  ...        1      2\n",
              "7                2                        1  ...        1      2\n",
              "8                2                        1  ...        5      2\n",
              "9                4                        2  ...        1      2\n",
              "\n",
              "[10 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dljNIAqgN9bu"
      },
      "source": [
        "### Replace missing data by column mode\n",
        "\n",
        "Pandas considers values like `NaN` and `None` to represent missing data. The `pandas.isnull` function can be used to tell whether or not a value is missing. \n",
        "\n",
        "Let's use `apply()` across all of the columns in our DataFrame to figure out which values are missing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvUROVQKN9bv"
      },
      "source": [
        "empty = data.apply(lambda col: pd.isnull(col))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f97WeVCWN9by",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "1da183e9-71dc-4b99-a9f4-446f7f0f7306"
      },
      "source": [
        "empty.sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Clump Thickness                 0\n",
              "Uniformity of Cell Size         0\n",
              "Uniformity of Cell Shape        0\n",
              "Marginal Adhesion               0\n",
              "Single Epithelial Cell Size     0\n",
              "Bare Nuclei                    16\n",
              "Bland Chromatin                 0\n",
              "Normal Nucleoli                 0\n",
              "Mitoses                         0\n",
              "Class                           0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFKcAB9DN9b0"
      },
      "source": [
        "Observe that one of the columns has 16 missing entries. Lets replace them with the mode of the column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZgIMbLnN9b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "37745ada-c944-4993-c9d9-f36c58cf7a46"
      },
      "source": [
        "data['Bare Nuclei'].mode()[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hG9BuA_zN9b5"
      },
      "source": [
        "data['Bare Nuclei'].fillna(data['Bare Nuclei'].mode()[0], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHNYgujrN9b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "32e8954b-0483-4e23-80bb-0ecea695b096"
      },
      "source": [
        "empty = data.apply(lambda col: pd.isnull(col))\n",
        "empty.sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Clump Thickness                0\n",
              "Uniformity of Cell Size        0\n",
              "Uniformity of Cell Shape       0\n",
              "Marginal Adhesion              0\n",
              "Single Epithelial Cell Size    0\n",
              "Bare Nuclei                    0\n",
              "Bland Chromatin                0\n",
              "Normal Nucleoli                0\n",
              "Mitoses                        0\n",
              "Class                          0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7TYXV_SN9b-"
      },
      "source": [
        "###Â Create a binary class label\n",
        "\n",
        "To do so, we will create a simple function to convert the label and store the result in a new colum of the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r324eES9N9b_"
      },
      "source": [
        "def convert_to_binary(l):\n",
        "    return (l-2.0)/2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZT_2zYCN9cB"
      },
      "source": [
        "# We create the new column\n",
        "\n",
        "data['Binary Class'] = data[['Class']].apply(lambda l: convert_to_binary(l))\n",
        "\n",
        "data.drop('Class',axis=1,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpjuntFBN9cE"
      },
      "source": [
        "Lets take a look to the histogram of the binary class label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1_G8zwzN9cF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "6b3590f2-2204-4df9-a222-c8389b438600"
      },
      "source": [
        "data['Binary Class'].hist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fb5b86c0b38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPJOAVqXN9cI"
      },
      "source": [
        "### Create a train and test sets\n",
        "\n",
        "We will simply split the dataset at random. 80% of the data will go to the training set, the rest to the test set. We will use [Numpy's random permutation function](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.permutation.html) and the `.iloc()` method. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ny8O9BOdN9cJ"
      },
      "source": [
        "frac_train = 0.7\n",
        "\n",
        "data_train = int(data.shape[0]*frac_train) # Training points\n",
        "\n",
        "np.random.seed(seed=10) #To fix the random seed. So we all get the same partition\n",
        "\n",
        "mask = np.random.permutation(data.shape[0]) # Random order of data indexes\n",
        "\n",
        "train_data = data.iloc[list(mask[:data_train])].copy()\n",
        "\n",
        "test_data = data.iloc[list(mask[data_train:])].copy()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUDJmwnZN9cM"
      },
      "source": [
        "Note that we store the train and test data in separate Dataframes, so we can normalize them without modifying the oringinal data (We could add more columns to the original Dataframe, but this would be later on annoying to index)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxPfl4TvN9cN"
      },
      "source": [
        "### Normalize input variables \n",
        "\n",
        "In general, it is not recommended that input variables (or features) are defined in different ranges. I.e., $x_1$ for instance takes values in the $[-1,1]$ set and $x_2$ takes values in the $[-10^6,10^6]$ range. \n",
        "\n",
        "To improve both the numerical robustness of our estimators and **enhance interpretability**, we will apply a linear normalization preprocessing stage to use as input to the linear regression features with (sample) mean 0 and (sample) variance equal to 1. \n",
        "\n",
        "Given the unnormalized feature matrix $\\mathbf{X}_{N\\times D}$, we compute the sample mean per feature column, $\\mu_j$, and the sample variance per  feature column, $\\sigma^2_j$ for $j=1,\\ldots,D$. Then, each row of the normalized feature matrix $\\overline{\\mathbf{X}}_{N\\times D}$ is obtained as follows:\n",
        "\n",
        "$$\\overline{\\mathbf{x}}^{(i)}= \\left[\\frac{x_1^{(i)}-\\mu_1}{\\sqrt{\\sigma^2_1}}, ~~ \\frac{x_1^{(i)}-\\mu_2}{\\sqrt{\\sigma^2_2}}, \\ldots, \\frac{x_1^{(D)}-\\mu_1}{\\sqrt{\\sigma^2_D}}\\right]$$\n",
        "\n",
        "**Note this does not affect to the logistic regression solution**, we are simply re-scaling the parameter vector $\\boldsymbol{w}$. Using this scaling, we can now effectively compare the effect that each variable has in the logistic regression solution: **the larger $|w_j|$ is, the more effect the $j$-th feature has in the estimation of the class**.\n",
        "\n",
        "One important aspect is that the test set is normalized **using the train set statistics (mean and variance)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNSRoGOlN9cN"
      },
      "source": [
        "means = train_data.mean()\n",
        "stds = train_data.std()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juoVFMevN9cR"
      },
      "source": [
        "for column_name in train_data.keys()[:-1]:\n",
        "    train_data[column_name] = (train_data[column_name]-means[column_name])/(stds[column_name])\n",
        "    test_data[column_name] = (test_data[column_name]-means[column_name])/(stds[column_name])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P91TSg9lN9cW"
      },
      "source": [
        "Lets take a look to the resulting dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pOjVm0HN9cY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "369934be-276b-46f1-8439-64c6d6175930"
      },
      "source": [
        "train_data.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Clump Thickness</th>\n",
              "      <th>Uniformity of Cell Size</th>\n",
              "      <th>Uniformity of Cell Shape</th>\n",
              "      <th>Marginal Adhesion</th>\n",
              "      <th>Single Epithelial Cell Size</th>\n",
              "      <th>Bare Nuclei</th>\n",
              "      <th>Bland Chromatin</th>\n",
              "      <th>Normal Nucleoli</th>\n",
              "      <th>Mitoses</th>\n",
              "      <th>Binary Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>285</th>\n",
              "      <td>1.312402</td>\n",
              "      <td>2.290172</td>\n",
              "      <td>2.279400</td>\n",
              "      <td>2.403579</td>\n",
              "      <td>2.253071</td>\n",
              "      <td>1.762256</td>\n",
              "      <td>2.637255</td>\n",
              "      <td>1.314188</td>\n",
              "      <td>0.839391</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>607</th>\n",
              "      <td>-1.215652</td>\n",
              "      <td>-0.697983</td>\n",
              "      <td>-0.748562</td>\n",
              "      <td>-0.651924</td>\n",
              "      <td>-0.545621</td>\n",
              "      <td>-0.699086</td>\n",
              "      <td>-1.020658</td>\n",
              "      <td>-0.606045</td>\n",
              "      <td>-0.343497</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>587</th>\n",
              "      <td>0.228950</td>\n",
              "      <td>-0.697983</td>\n",
              "      <td>-0.748562</td>\n",
              "      <td>-0.651924</td>\n",
              "      <td>-0.545621</td>\n",
              "      <td>-0.699086</td>\n",
              "      <td>-0.614223</td>\n",
              "      <td>-0.286006</td>\n",
              "      <td>-0.343497</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>458</th>\n",
              "      <td>0.228950</td>\n",
              "      <td>-0.697983</td>\n",
              "      <td>-0.412122</td>\n",
              "      <td>-0.651924</td>\n",
              "      <td>-0.545621</td>\n",
              "      <td>-0.699086</td>\n",
              "      <td>-1.020658</td>\n",
              "      <td>-0.606045</td>\n",
              "      <td>-0.343497</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>431</th>\n",
              "      <td>0.228950</td>\n",
              "      <td>-0.697983</td>\n",
              "      <td>-0.748562</td>\n",
              "      <td>0.027077</td>\n",
              "      <td>0.387276</td>\n",
              "      <td>-0.699086</td>\n",
              "      <td>-0.207789</td>\n",
              "      <td>-0.286006</td>\n",
              "      <td>-0.343497</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>2.034704</td>\n",
              "      <td>-0.033949</td>\n",
              "      <td>0.933639</td>\n",
              "      <td>-0.312424</td>\n",
              "      <td>-0.079172</td>\n",
              "      <td>0.394844</td>\n",
              "      <td>0.198646</td>\n",
              "      <td>2.274304</td>\n",
              "      <td>0.247947</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>560</th>\n",
              "      <td>0.228950</td>\n",
              "      <td>-0.697983</td>\n",
              "      <td>-0.748562</td>\n",
              "      <td>-0.651924</td>\n",
              "      <td>-0.545621</td>\n",
              "      <td>-0.699086</td>\n",
              "      <td>-0.207789</td>\n",
              "      <td>-0.606045</td>\n",
              "      <td>-0.343497</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>445</th>\n",
              "      <td>-0.854502</td>\n",
              "      <td>-0.697983</td>\n",
              "      <td>-0.748562</td>\n",
              "      <td>-0.651924</td>\n",
              "      <td>-0.545621</td>\n",
              "      <td>-0.699086</td>\n",
              "      <td>-1.020658</td>\n",
              "      <td>-0.606045</td>\n",
              "      <td>-0.343497</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>292</th>\n",
              "      <td>1.312402</td>\n",
              "      <td>1.626138</td>\n",
              "      <td>1.606520</td>\n",
              "      <td>-0.651924</td>\n",
              "      <td>-0.545621</td>\n",
              "      <td>-0.699086</td>\n",
              "      <td>1.011516</td>\n",
              "      <td>2.274304</td>\n",
              "      <td>-0.343497</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>579</th>\n",
              "      <td>-1.215652</td>\n",
              "      <td>-0.697983</td>\n",
              "      <td>-0.748562</td>\n",
              "      <td>-0.651924</td>\n",
              "      <td>-0.545621</td>\n",
              "      <td>-0.699086</td>\n",
              "      <td>-0.207789</td>\n",
              "      <td>-0.606045</td>\n",
              "      <td>-0.343497</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Clump Thickness  Uniformity of Cell Size  ...   Mitoses  Binary Class\n",
              "285         1.312402                 2.290172  ...  0.839391           1.0\n",
              "607        -1.215652                -0.697983  ... -0.343497           0.0\n",
              "587         0.228950                -0.697983  ... -0.343497           0.0\n",
              "458         0.228950                -0.697983  ... -0.343497           0.0\n",
              "431         0.228950                -0.697983  ... -0.343497           0.0\n",
              "52          2.034704                -0.033949  ...  0.247947           1.0\n",
              "560         0.228950                -0.697983  ... -0.343497           0.0\n",
              "445        -0.854502                -0.697983  ... -0.343497           0.0\n",
              "292         1.312402                 1.626138  ... -0.343497           1.0\n",
              "579        -1.215652                -0.697983  ... -0.343497           0.0\n",
              "\n",
              "[10 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-bxDdx1N9cb"
      },
      "source": [
        "## Part II. Tensors in Pytorch\n",
        "\n",
        "\n",
        "Note: a big part of the following material is a personal wrap-up of [Facebook's Deep Learning Course in Udacity](https://www.udacity.com/course/deep-learning-pytorch--ud188). So all credit goes for them!!\n",
        "\n",
        "PyTorch in a lot of ways behaves like  Numpy arrays. PyTorch takes these tensors and makes it simple to move them to GPUs for the faster processing needed when training neural networks. It also provides a module that automatically calculates gradients and another module specifically for building neural networks. \n",
        "\n",
        "It turns out neural network computations are just a bunch of linear algebra operations on *tensors*, a generalization of matrices. A vector is a 1-dimensional tensor, a matrix is a 2-dimensional tensor, an array with three indices is a 3-dimensional tensor (RGB color images for example). The fundamental data structure for neural networks are tensors and PyTorch (as well as pretty much every other deep learning framework) is built around tensors.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYvnINsvN9cc"
      },
      "source": [
        "### Creating and manipulating tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIyBC7CyN9cc"
      },
      "source": [
        "# First, import PyTorch\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvQLWjzfN9cf"
      },
      "source": [
        "Lets create a function to manipulate torch tensors ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlf3Z3khN9cg"
      },
      "source": [
        "def activation(x):\n",
        "    \"\"\" Sigmoid activation function \n",
        "    \n",
        "        Arguments\n",
        "        ---------\n",
        "        x: torch.Tensor\n",
        "    \"\"\"\n",
        "    return 1/(1+torch.exp(-x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "httFutNwN9ck"
      },
      "source": [
        "### Generate some data\n",
        "torch.manual_seed(7) # Set the random seed so things are predictable\n",
        "\n",
        "# Features are 5 random normal variables\n",
        "features = torch.randn((1, 5))\n",
        "# True weights for our data, random normal variables again\n",
        "weights = torch.randn_like(features)\n",
        "# and a true bias term\n",
        "bias = torch.randn((1, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1NGjJcWN9cm"
      },
      "source": [
        "Above we generated data we can use to get the output of our simple network. This is all just random for now, going forward we'll start using normal data. Going through each relevant line:\n",
        "\n",
        "`features = torch.randn((1, 5))` creates a tensor with shape `(1, 5)`, one row and five columns, that contains values randomly distributed according to the normal distribution with a mean of zero and standard deviation of one. \n",
        "\n",
        "`weights = torch.randn_like(features)` creates another tensor with the same shape as `features`, again containing values from a normal distribution.\n",
        "\n",
        "Finally, `bias = torch.randn((1, 1))` creates a single value from a normal distribution.\n",
        "\n",
        "PyTorch tensors can be added, multiplied, subtracted, etc, just like Numpy arrays. In general, you'll use PyTorch tensors pretty much the same way you'd use.\n",
        "\n",
        "> **Exercise**: Using the `activation()` function, calculate the output a binary logistic regressor with input features `features`, weights `weights`, and bias `bias`. Similar to Numpy, PyTorch has a [`torch.sum()`](https://pytorch.org/docs/stable/torch.html#torch.sum) function, as well as a `.sum()` method on tensors, for taking sums. Use the function `activation` defined above as the activation function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVLnZ7TNN9cn"
      },
      "source": [
        "###Â YOUR CODE HERE\n",
        "\n",
        "y = activation(torch.mm(features, torch.t(weights) + bias))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8qaJpkBN9cs"
      },
      "source": [
        "You can do the multiplication and sum in the same operation using a matrix multiplication. In general, you'll want to use matrix multiplications since they are more efficient and accelerated using modern libraries and high-performance computing on GPUs.  For this we can use [`torch.mm()`](https://pytorch.org/docs/stable/torch.html#torch.mm) or [`torch.matmul()`](https://pytorch.org/docs/stable/torch.html#torch.matmul) which is somewhat more complicated and supports broadcasting.\n",
        "\n",
        "First, check the dimension of `features` and `weights` using the method `.shape`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtpDuXAnN9ct",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "46392509-185c-47d5-aacb-96cf76c4dfd2"
      },
      "source": [
        "print(features.shape)\n",
        "print(weights.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 5])\n",
            "torch.Size([1, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQsuxO29N9cv"
      },
      "source": [
        "> **Exercise**: See what comes if you try to directly multiply both tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmSJEIXTN9cw"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "#prod= torch.mm(features, weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8DFPmcYN9cy"
      },
      "source": [
        "What's happening here is our tensors aren't the correct shapes to perform a matrix multiplication. There are a few options here: [`weights.reshape()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.reshape), [`weights.resize_()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.resize_), and [`weights.view()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view).\n",
        "\n",
        "* `weights.reshape(a, b)` will return a new tensor with the same data as `weights` with size `(a, b)` sometimes, and sometimes a clone, as in it copies the data to another part of memory.\n",
        "* `weights.resize_(a, b)` returns the same tensor with a different shape. However, if the new shape results in fewer elements than the original tensor, some elements will be removed from the tensor (but not from memory). If the new shape results in more elements than the original tensor, new elements will be uninitialized in memory. Here I should note that the underscore at the end of the method denotes that this method is performed **in-place**. Here is a great forum thread to [read more about in-place operations](https://discuss.pytorch.org/t/what-is-in-place-operation/16244) in PyTorch.\n",
        "* `weights.view(a, b)` will return a new tensor with the same data as `weights` with size `(a, b)`.\n",
        "\n",
        "Any of the three methods will work for this. So, now we can reshape `weights` to have five rows and one column with something like `weights.view(5, 1)`.\n",
        "\n",
        "\n",
        "> **Exercise**: Calculate the output of the logistic regressor using matrix multiplication."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WepbuXlIN9cz"
      },
      "source": [
        "y = activation(torch.mm(features, weights.view(5,1)) + bias)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1NdxVf3N9c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8e2b9d5d-8114-40a3-b565-936afc7f270e"
      },
      "source": [
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.1595]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DO7YlNfHN9c6"
      },
      "source": [
        "### Numpy to Torch and back\n",
        "\n",
        "PyTorch has a feature for converting between Numpy arrays and Torch tensors. To create a tensor from a Numpy array, use `torch.from_numpy()`. To convert a tensor to a Numpy array, use the `.numpy()` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YqVH7jnN9c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "98869a88-194d-4573-b57e-da19ba996fe9"
      },
      "source": [
        "a = np.random.rand(4,3)\n",
        "a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.29507589, 0.18333855, 0.01973246],\n",
              "       [0.74332123, 0.0382934 , 0.43367967],\n",
              "       [0.83208476, 0.01345856, 0.50933539],\n",
              "       [0.4795487 , 0.02965494, 0.01142853]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WztxUlemN9dB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "9093627a-975e-416d-e0b6-534105caef26"
      },
      "source": [
        "b = torch.from_numpy(a)\n",
        "b"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2951, 0.1833, 0.0197],\n",
              "        [0.7433, 0.0383, 0.4337],\n",
              "        [0.8321, 0.0135, 0.5093],\n",
              "        [0.4795, 0.0297, 0.0114]], dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYVmPhFZN9dD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "64230eb4-5df0-4bc0-808c-f4503e994d9e"
      },
      "source": [
        "b.numpy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.29507589, 0.18333855, 0.01973246],\n",
              "       [0.74332123, 0.0382934 , 0.43367967],\n",
              "       [0.83208476, 0.01345856, 0.50933539],\n",
              "       [0.4795487 , 0.02965494, 0.01142853]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnZFUJrbN9dE"
      },
      "source": [
        "The memory is shared between the Numpy array and Torch tensor, so if you change the values in-place of one object, the other will change as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGHVX1uIN9dF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "cf18a172-a7c3-4c86-a83b-a20283db1fa8"
      },
      "source": [
        "# Multiply PyTorch Tensor by 2, in place (Note the _)\n",
        "b.mul_(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5902, 0.3667, 0.0395],\n",
              "        [1.4866, 0.0766, 0.8674],\n",
              "        [1.6642, 0.0269, 1.0187],\n",
              "        [0.9591, 0.0593, 0.0229]], dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwyLUIlEN9dI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "0073f3f2-f009-4a62-9759-f27e2c453308"
      },
      "source": [
        "# Numpy array matches new values from Tensor\n",
        "a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.59015179, 0.3666771 , 0.03946492],\n",
              "       [1.48664246, 0.07658681, 0.86735934],\n",
              "       [1.66416952, 0.02691712, 1.01867078],\n",
              "       [0.9590974 , 0.05930988, 0.02285705]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IoATd7aN9dL"
      },
      "source": [
        "## Part III. Define a logistc regression and its loss function in Pytorch\n",
        "\n",
        "#### The `nn.()` module\n",
        "\n",
        "PyTorch provides a module `nn` that makes building neural networks much simpler. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcYEaECgN9dM"
      },
      "source": [
        "from torch import nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwJBPvRXN9dP"
      },
      "source": [
        "You should to get use to inspect [Pytorch official documentation](https://pytorch.org/docs/stable/index.html) to understand the structure and usability of methods and classes required to work with Pytorch and train neural networks.\n",
        "\n",
        "With the following code I show you how to create a Logistic Regression network whose parameters will be later on optimized for our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxgnKojiN9dQ"
      },
      "source": [
        "class LR(nn.Module):\n",
        "    def __init__(self,dimx):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.weights = nn.Parameter(torch.randn(dimx,1),requires_grad = True)\n",
        "        \n",
        "        self.bias = nn.Parameter(torch.randn(1,1),requires_grad = True)\n",
        "        \n",
        "        # Define sigmoid activation and softmax output \n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Pass the input tensor through each of our operations\n",
        "        x = self.sigmoid(torch.matmul(x,self.weights)+self.bias)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ck8mpNMhN9dR"
      },
      "source": [
        "Let's go through this bit by bit.\n",
        "\n",
        "```python\n",
        "class Network(nn.Module):\n",
        "```\n",
        "\n",
        "Here we're inheriting from `nn.Module`. Combined with `super().__init__()` this creates a class that tracks the architecture and provides a lot of useful methods and attributes. It is mandatory to inherit from `nn.Module` when you're creating a class for your network. The name of the class itself can be anything.\n",
        "\n",
        "```python\n",
        "self.weights = nn.Parameter(torch.randn(dimx,1),requires_grad = True)\n",
        "self.bias = nn.Parameter(torch.randn(1,1),requires_grad = True)\n",
        "```\n",
        "\n",
        "The Parameter class defines a kind of Tensor that is to be considered a module parameter. Namely, we will optimize the values of these tensors. Note that we define a random optimization.\n",
        "\n",
        "```python\n",
        "def forward(self, x):\n",
        "```\n",
        "\n",
        "PyTorch networks created with `nn.Module` must have a `forward` method defined. It takes in a tensor `x` and passes it through the operations you defined in the `__init__` method.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cu5qP2uN9dS"
      },
      "source": [
        "### A more compact definition of the network using pre-defined layers\n",
        "\n",
        "The `.nn` package provides us with predefined layers and operators so that the implementation of our networks is indeed much easier (and compact). The following code performs exactly as the class `LR` defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ktJIr7ON9dU"
      },
      "source": [
        "class LR2(nn.Module):\n",
        "    def __init__(self,dimx):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.output = nn.Linear(dimx,1)\n",
        "    \n",
        "        # Define sigmoid activation and softmax output \n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Pass the input tensor through each of our operations\n",
        "        x = self.output(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pp9uRcTeN9dW"
      },
      "source": [
        "When we define a logistic regressor by instantiating the class `LR2`, we can access the weights and the bias as `self.output.weight` and `self.output.bias`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEyYUPueN9dX"
      },
      "source": [
        "#### Evaluate the logistic regressor using our data. The autograd module\n",
        "\n",
        "Lets take the first 10 datapoints in the training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NV76TLLPN9dZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "fe7b2c37-8ba6-460d-a0c8-d4c851ed6f79"
      },
      "source": [
        "x = np.array(train_data[:10]).astype(np.float32)\n",
        "y = x[:,-1]    # Last Column is the class\n",
        "x = x[:,:-1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1ca57f4c7e3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m    \u001b[0;31m# Last Column is the class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wrbp08GN9dg"
      },
      "source": [
        "> **Exercise:** Instantiate the class `LR` or `LR2` with the right input dimension"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gl50a5juN9dh"
      },
      "source": [
        "## YOUR CODE HERE\n",
        "\n",
        "dimx=x.shape[1]\n",
        "my_classifier = LR(dimx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alo5K_yMN9dj"
      },
      "source": [
        "To evaluate the logistic regressor output for the data in `x`, we make use of the class `forward` method. Do not forget to feed the function with a **torch tensor**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70jZYfbiN9dj"
      },
      "source": [
        "output = my_classifier.forward(torch.tensor(x)) # estos nums son probabilidades"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEJfmZ3cN9dl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "88e860a6-59c5-42bd-fa4c-d9ec6557314f"
      },
      "source": [
        "print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1.0000],\n",
            "        [0.0122],\n",
            "        [0.0343],\n",
            "        [0.0159],\n",
            "        [0.2592],\n",
            "        [0.9999],\n",
            "        [0.0173],\n",
            "        [0.0128],\n",
            "        [0.9866],\n",
            "        [0.0143]], grad_fn=<SigmoidBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSwUY7cmN9do"
      },
      "source": [
        "Above we can see the operation that created `output`, a sigmoid operation `SigmoidBackward`. The autograd module keeps track of all operations that affect Parameter tensors and knows how to calculate the gradient for each one. In this way, it's able to calculate the gradients for a chain of operations, with respect to any one tensor. Let's reduce the tensor `y` to a scalar value, the binary cross entropy. We will use the class `nn.BCE()`. See how to use the function in the [official documentation](https://pytorch.org/docs/stable/nn.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1qgQol2N9do"
      },
      "source": [
        "bce = nn.BCELoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HYtpMEQN9ds",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eb23a79b-7d6a-4244-feb4-b218ee5be603"
      },
      "source": [
        "loss = bce(output,torch.tensor(y).view(10,1))\n",
        "\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.0422, grad_fn=<BinaryCrossEntropyBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bnr3MuVMN9du"
      },
      "source": [
        "Above we can see the operation that created `loss`, a binary cross entropy `BinaryCrossEntropyBackward`.\n",
        "\n",
        "\n",
        "Now that we know how to calculate a loss, Torch provides a module, `autograd`, for automatically calculating the gradients of the loss w.r.t. the tensors. Autograd works by keeping track of operations performed on tensors, then going backwards through those operations, calculating gradients along the way.\n",
        "To make sure PyTorch keeps track of operations on a tensor and calculates the gradients, you need to set `requires_grad = True` on a tensor. You can do this at creation with the `requires_grad` keyword, or at any time with `x.requires_grad_(True)`. By default, all tensors created by instantiating the class `nn.Parameter` have `requires_grad = True`.\n",
        "\n",
        "> **Exercise**: Check that both the weights and the bias of the class `LR` and `LR2` have this flag set to `True`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EU0Y2UGgN9dv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "fdc2b373-7f92-4da8-9b34-4549d9de6a11"
      },
      "source": [
        "#YOUR CODE HERE \n",
        "print(my_classifier.weights.requires_grad)\n",
        "print(my_classifier.bias.requires_grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-X-zXgV0N9dx"
      },
      "source": [
        "You can turn off gradients for a block of code with the `torch.no_grad()` content:\n",
        "```python\n",
        "x = torch.zeros(1, requires_grad=True)\n",
        ">>> with torch.no_grad():\n",
        "...     y = x * 2\n",
        ">>> y.requires_grad\n",
        "False\n",
        "```\n",
        "\n",
        "Also, you can turn on or off gradients altogether with `torch.set_grad_enabled(True|False)`. Turning off gradients is recommended for instance when you are just evaluating you network, i.e., when training is finished and only parameter evaluation is required.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXXCZyf7N9dx"
      },
      "source": [
        "### The `.backward()` operator\n",
        "\n",
        "\n",
        "The gradients are computed with respect to some variable `z` with `z.backward()`. This does a backward pass through the operations that created `z`. This means that when we calculate the loss and call `loss.backward()`, the gradients for the parameters are calculated. These gradients are used to update the weights with gradient descent.\n",
        "\n",
        "After calling  `loss.backward()` the gradient of `loss` w.r.t. to any tensor `x` can be accessed as `x.grad`\n",
        "\n",
        "\n",
        "> **Exercise**: Given the tensor $x$ (of size $10\\times1$) defined below, evaluate the gradient of $y=x^T~x$ w.r.t. every component of $x$. Does it coincide with the analytical solution?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tW1nq2BRN9dz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "outputId": "297f28b6-1a02-4cda-e6e3-7801af44f8a0"
      },
      "source": [
        "x = torch.randn(10,1,requires_grad=True)\n",
        "\n",
        "#YOUR CODE HERE\n",
        "\n",
        "y = torch.mm(x.view(1,10),x)\n",
        "\n",
        "print(y.grad_fn)\n",
        "\n",
        "#YOUR CODE HERE\n",
        "y.backward()\n",
        "\n",
        "print(x.grad)  #dy/dx\n",
        "\n",
        "\n",
        "# Turn off gradients for validation, saves memory and computations\n",
        "with torch.no_grad():\n",
        "    #YOUR CODE HERE. Plot the computed gradient vs 2*x (the analytical solution)\n",
        "    plt.stem(x.grad.detach().numpy(),label='Autograd')\n",
        "    plt.plot(2*x.detach().numpy(),'-g',label='$2x$')\n",
        "    plt.legend()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<MmBackward object at 0x7fb56c194978>\n",
            "tensor([[-1.8359],\n",
            "        [-0.9156],\n",
            "        [-1.4489],\n",
            "        [ 2.5598],\n",
            "        [-1.9881],\n",
            "        [ 3.6299],\n",
            "        [-1.2057],\n",
            "        [ 3.2296],\n",
            "        [ 3.8604],\n",
            "        [-0.8450]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: In Matplotlib 3.3 individual lines on a stem plot will be added as a LineCollection instead of individual lines. This significantly improves the performance of a stem plot. To remove this warning and switch to the new behaviour, set the \"use_line_collection\" keyword argument to True.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noigm9/8Bk/cvlUwlpeEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6vg8LBVN9d1"
      },
      "source": [
        "`.detach()` method constructs a new view on a tensor which is declared not to need gradients. You can see a nice example in this [link](method constructs a new view on a tensor which is declared not to need gradients). Typically, we will use detach to export values to numpy to perform model evaluation. In Pytorch, you cannot call the `.numpy()` operators in Tensor that have `require_grad=True`.\n",
        "\n",
        "\n",
        "An important aspects to implement Gradient Descent Loops is that if call again the `.backward()` operator, then **gradients are accumulated** in the variable `x.grad`. \n",
        "\n",
        "> **Exercise**: Check that gradients are accumulated by calling again `y.backward()`   \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4qWJubVN9d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "outputId": "7221e706-8979-4aba-cc35-ddd52d5becb1"
      },
      "source": [
        "y = torch.matmul(x.view(1,10),x)\n",
        "#YOUR CODE HERE\n",
        "print(y.grad_fn) #first we use this backward function to calculate the gradient\n",
        "\n",
        "y.backward()\n",
        "\n",
        "#YOUR CODE HERE. Plot the computed gradient vs 2*x (the analytical solution)\n",
        "with torch.no_grad():\n",
        "     #YOUR CODE HERE. Plot the computed gradient vs 2*x (the analytical solution)\n",
        "    plt.stem(x.grad.detach().numpy(),label='Autograd')\n",
        "    plt.plot(2*x.detach().numpy(),'-g',label='$2x$')\n",
        "    plt.legend()\n",
        "     \n",
        "#esto era para ver que se acumulan los gradientes\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<MmBackward object at 0x7fb56b0dc7b8>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: In Matplotlib 3.3 individual lines on a stem plot will be added as a LineCollection instead of individual lines. This significantly improves the performance of a stem plot. To remove this warning and switch to the new behaviour, set the \"use_line_collection\" keyword argument to True.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzU9LhjzN9d5"
      },
      "source": [
        "In order to reset gradients, we will see that the Pytorch optimizers provide us with the appropiate method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXi146U3N9d6"
      },
      "source": [
        "## Part IV. Train the logistic regressor with the complete dataset\n",
        "\n",
        "When we create a network with PyTorch, all of the parameters are initialized with `requires_grad = True`. This means that when we calculate the loss and call `loss.backward()`, the gradients for the parameters are calculated. These gradients are used to update the weights with gradient descent. Below you can see an example of calculating the gradients using a backwards pass."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDfC7T2FN9d7"
      },
      "source": [
        "# We use all data points  \n",
        "\n",
        "x = np.array(train_data).astype(np.float32)\n",
        "# Last Column is the class\n",
        "y = x[:,-1]  \n",
        "x = x[:,:-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ou8TnLeuN9d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "3a09623e-2bbd-4c08-959e-08e5f1d72f04"
      },
      "source": [
        "my_classifier2 = LR2(x.shape[1])\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "output = my_classifier2.forward(torch.tensor(x))\n",
        "\n",
        "loss = criterion(output,torch.tensor(y))\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "print(my_classifier2.output.weight.grad.shape)  #dloss/dw shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 9])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([489])) that is different to the input size (torch.Size([489, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QtuD9WkN9d-"
      },
      "source": [
        "#### Defining an optimizer\n",
        "\n",
        "There's one last piece we need to start training, an optimizer that we'll use to update the weights with the gradients. We get these from PyTorch's [`optim` package](https://pytorch.org/docs/stable/optim.html). For example we can use stochastic gradient descent with `optim.SGD`. You can see how to define an optimizer below.\n",
        "\n",
        "With the following code, we can define the optimizer:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NSPGPsAN9d_"
      },
      "source": [
        "from torch import optim\n",
        "\n",
        "# Optimizers require the parameters to optimize and a learning rate\n",
        "optimizer = optim.SGD(my_classifier2.parameters(), lr=0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vit2uFiBN9eA"
      },
      "source": [
        "To perform a SGD iteration, we simply call `optmizer.step()`\n",
        "\n",
        "\n",
        "By the way, `my_classifier2.parameters()` returns a [generator object](https://realpython.com/introduction-to-python-generators/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3jNtcDSN9eB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "14ca2cbb-e658-4e98-b7bd-f0dcf3394106"
      },
      "source": [
        "params = my_classifier2.parameters()\n",
        "\n",
        "print(type(params))\n",
        "\n",
        "print(next(params))\n",
        "\n",
        "print(next(params))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'generator'>\n",
            "Parameter containing:\n",
            "tensor([[-0.1130,  0.0131, -0.1902, -0.1456,  0.1120,  0.1918,  0.0047, -0.1297,\n",
            "          0.1825]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.1827], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxvNRrVMN9eC"
      },
      "source": [
        "If we call again the method `next()` over params, Python will yield a `StopIteration` error:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwVPBvKuN9eD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "7c2c2059-eae5-4fce-a970-e723bc948667"
      },
      "source": [
        "print(next(params))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "StopIteration",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-419f69b3edd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mStopIteration\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybk7XyipN9eF"
      },
      "source": [
        "#### Creating a class with both the network and a training method\n",
        "\n",
        "Object Oriented Programming (OOP) in Python is a versatil and convenient method to implement our own software and create reusable sofware. Indeed, along this whole notebook, we have been using classes all the time.\n",
        "\n",
        "[Here](https://www.programiz.com/python-programming/object-oriented-programming) you can find a short introduction to OOP in Python.\n",
        "\n",
        "With the following code, we will define a class that incorporates the definition of the logistic regression network and **a method to train the parameters**. Go carefully through the code and try to understand it line by line."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3JxmgOAN9eG"
      },
      "source": [
        "''' This class inherits from the LR2 class. So it has the same atributes\n",
        "and methods, and some others that we will add. \n",
        "'''\n",
        "class LR_extended(LR2):\n",
        "    \n",
        "    def __init__(self,dimx,sgd_iterations=1000,lr=0.001):\n",
        "        \n",
        "        super().__init__(dimx)  #To initialize LR2!\n",
        "        \n",
        "        self.lr = lr #Learning Rate\n",
        "        \n",
        "        self.optim = optim.SGD(self.parameters(), self.lr)\n",
        "        \n",
        "        self.sgd_iterations = sgd_iterations #SGD steps\n",
        "        \n",
        "        self.criterion = nn.BCELoss()\n",
        "        \n",
        "        # A list to store the loss evolution along training\n",
        "        \n",
        "        self.loss_during_training = [] \n",
        "        \n",
        "    def train(self,x,y):\n",
        "        \n",
        "        # SGD Loop\n",
        "        \n",
        "        for iter in range(int(self.sgd_iterations)):\n",
        "        \n",
        "            self.optim.zero_grad()  #TO RESET GRADIENTS!\n",
        "\n",
        "            out = self.forward(x)\n",
        "\n",
        "            loss = self.criterion(out,y)\n",
        "\n",
        "            self.loss_during_training.append(loss.item())\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            self.optim.step()\n",
        "        \n",
        "            if(iter % 1000 == 0): # Every 1000 iterations\n",
        "                \n",
        "                print(\"Training loss after %d iterations: %f\" \n",
        "                      %(iter,self.loss_during_training[-1]))\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D99WP9lTN9eK"
      },
      "source": [
        "Lets instantiate the class and train the logistic regressor. See how compact and easy is now (OOP is great!)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yELwP6vHN9eL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "35b46ccc-71ed-4ab2-dcae-7fe1147222df"
      },
      "source": [
        "my_LR = LR_extended(x.shape[1],sgd_iterations=3000,lr=0.001)\n",
        "\n",
        "my_LR.train(torch.tensor(x),torch.tensor(y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([489])) that is different to the input size (torch.Size([489, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training loss after 0 iterations: 0.584372\n",
            "Training loss after 1000 iterations: 0.269989\n",
            "Training loss after 2000 iterations: 0.194411\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOjI6g9cN9eN"
      },
      "source": [
        "Note that if you believe that training isn't finished yet, you can simply call again the `.train()` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "povj6vhON9eO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "34116869-04bd-4924-f6c6-5aed6bd17aca"
      },
      "source": [
        "my_LR.train(torch.tensor(x),torch.tensor(y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([489])) that is different to the input size (torch.Size([489, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training loss after 0 iterations: 0.161354\n",
            "Training loss after 1000 iterations: 0.143000\n",
            "Training loss after 2000 iterations: 0.131391\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kkaYAvzN9eR"
      },
      "source": [
        "> **Exercise**: Train from scratch the logistic regresson for 20.000 iterations and plot the evolution of the binary cross entropy loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkV3ohVTN9eR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "outputId": "2fd63ea4-d904-4ca4-a789-daff81de08b4"
      },
      "source": [
        "# Your code here\n",
        "my_LR2 = LR_extended(x.shape[1],sgd_iterations=20000,lr=0.001)\n",
        "# Your code here\n",
        "my_LR2.train(torch.tensor(x),torch.tensor(y))\n",
        "\n",
        "\n",
        "plt.plot(my_LR2.loss_during_training,'-g',label='$Ev$')\n",
        "# el error baja mucho mas rapido el SGD que el GD"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([489])) that is different to the input size (torch.Size([489, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training loss after 0 iterations: 0.642660\n",
            "Training loss after 1000 iterations: 0.273910\n",
            "Training loss after 2000 iterations: 0.193057\n",
            "Training loss after 3000 iterations: 0.159021\n",
            "Training loss after 4000 iterations: 0.140529\n",
            "Training loss after 5000 iterations: 0.129000\n",
            "Training loss after 6000 iterations: 0.121162\n",
            "Training loss after 7000 iterations: 0.115502\n",
            "Training loss after 8000 iterations: 0.111229\n",
            "Training loss after 9000 iterations: 0.107893\n",
            "Training loss after 10000 iterations: 0.105217\n",
            "Training loss after 11000 iterations: 0.103021\n",
            "Training loss after 12000 iterations: 0.101186\n",
            "Training loss after 13000 iterations: 0.099629\n",
            "Training loss after 14000 iterations: 0.098290\n",
            "Training loss after 15000 iterations: 0.097125\n",
            "Training loss after 16000 iterations: 0.096101\n",
            "Training loss after 17000 iterations: 0.095193\n",
            "Training loss after 18000 iterations: 0.094383\n",
            "Training loss after 19000 iterations: 0.093653\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fb56af9c3c8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDb3mytsexBwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kGCeXe8N9eT"
      },
      "source": [
        "> **Exercise**: Print all model parameters (e.g. weights and bias)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOml274qN9eU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "e053e179-c57d-44d9-d335-cfa3623164e7"
      },
      "source": [
        "#Your code here\n",
        "\n",
        "print(my_LR2.output.weight)\n",
        "#Your code here\n",
        "print(my_LR2.output.bias)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[0.6317, 0.5438, 0.8766, 0.6921, 0.3496, 1.0436, 0.5338, 0.3506, 0.4953]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.6319], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAAltAOON9eW"
      },
      "source": [
        "#### Evaluating the model in the test set\n",
        "\n",
        "Now that we have trained our logistic regressor, it is time to compare its performance in both the training and the test sets.\n",
        "\n",
        "> **Exercise**: Compute the logistic regressor output for both the training data and the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27YuKe2yN9eW"
      },
      "source": [
        "out_train = my_LR.forward(torch.tensor(x))\n",
        "\n",
        "#Your code here\n",
        "xtest = np.array(test_data).astype(np.float32)\n",
        "# Last Column is the class\n",
        "ytest = xtest[:,-1]   \n",
        "xtest = xtest[:,:-1]\n",
        "\n",
        "out_test = my_LR.forward(torch.tensor(xtest))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tQVHYaAN9eY"
      },
      "source": [
        "Recall that the logistic regressor output is an estimated class probability. Assuming we put the threshold at a probability of 0.5, lets count the number of errors in both sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMcCDfh-N9eY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "0e2a5b0c-85d8-423b-ba7b-2c9c6754bcd1"
      },
      "source": [
        "error_rate_train = np.sum((out_train.detach().numpy()>=0.5) \n",
        "                          == y.reshape([-1,1]))/y.shape[0]\n",
        "\n",
        "error_rate_test = np.sum((out_test.detach().numpy()>=0.5) \n",
        "                         == ytest.reshape([-1,1]))/ytest.shape[0]\n",
        "\n",
        "print(error_rate_train)\n",
        "\n",
        "print(error_rate_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9652351738241309\n",
            "0.9666666666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilQNmwVQN9ea"
      },
      "source": [
        "### Part V. Training a Logistic Regressor with mini-batch stochastic gradient descent\n",
        "\n",
        "Finally, we are going to implement an scalable version of the gradient descent training implemented above. At every iteration, instead of evaluating the gradient using all data points, a small minibatch of data will be used.\n",
        "\n",
        "To make sure that all training data points are evenly used to evaluate the gradient, we will use a short iterating function. Instead of predefining a certain number of SGD iterations, we define a certain number of **epochs**. After every epoch **all datapoints** have been used in the optimizer once. \n",
        "\n",
        "\n",
        "> **Exercise**: Complete the code for the following class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgMpy8h0N9eb"
      },
      "source": [
        "''' This class inherits from the LR2 class. So it has the same atributes\n",
        "and methods, and some others that we will add. \n",
        "'''\n",
        "class LR_extended_minibatch(LR2):\n",
        "    \n",
        "    def __init__(self,dimx,num_train_data,\n",
        "                 epochs=100,lr=0.001,batch_size=50):\n",
        "        \n",
        "        super().__init__(dimx)  #To initialize LR2!\n",
        "        \n",
        "        self.lr = lr #Learning Rate\n",
        "        \n",
        "        self.optim = optim.SGD(self.parameters(), self.lr)\n",
        "        \n",
        "        self.epochs = epochs\n",
        "        \n",
        "        self.criterion = nn.BCELoss()\n",
        "        \n",
        "        # A list to store the loss evolution along training\n",
        "        \n",
        "        self.loss_during_training = [] \n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        self.num_train = num_train_data\n",
        "        \n",
        "        self.num_batchs = np.floor(self.num_train/self.batch_size)\n",
        "        \n",
        "    def train(self,x,y):\n",
        "        \n",
        "        # SGD Loop\n",
        "        \n",
        "        for e in range(int(self.epochs)):\n",
        "            \n",
        "            # Random data permutation at each epoch\n",
        "            \n",
        "            idx = np.random.permutation(self.num_train)\n",
        "            \n",
        "            running_loss = 0.\n",
        "            for i in range(int(self.num_batchs)):\n",
        "        \n",
        "                self.optim.zero_grad()  #TO RESET GRADIENTS!\n",
        "            \n",
        "                idx_batch = idx[i*self.batch_size:(i+1)*self.batch_size]\n",
        "\n",
        "                out = self.forward(x[idx_batch,:])\n",
        "\n",
        "                #Your code here\n",
        "                loss = criterion(out,y[idx_batch])\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "                #Your code here\n",
        "                loss.backward()\n",
        "                \n",
        "                self.optim.step()\n",
        "\n",
        "                \n",
        "            self.loss_during_training.append(running_loss/self.num_batchs)\n",
        "\n",
        "            if(e % 10 == 0): # Every 10 epochs\n",
        "\n",
        "                print(\"Training loss after %e epochs: %f\" \n",
        "                      %(e,self.loss_during_training[-1]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2z0VsKUN9ed"
      },
      "source": [
        "> **Exercise**: Train a logistic regressor with minibatch-stochastic gradient descent with different batch sizes (10,20,50 and 100) and plot the evolution of the loss function for all cases in the same plot. \n",
        "Also, compare with the evolution of the loss function when all data is used. Note that when all datapoints are used, every GD iteration correspond to a **full epoch**.\n",
        ">\n",
        "> Finally, observe if in all cases the parameters of the models (weights and biases) are similar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggFydXwwN9ei",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4481e680-47ed-4855-b41f-b2b9c4bbda9e"
      },
      "source": [
        "#Your code here\n",
        "\n",
        "\n",
        "my_LR_batches10 = LR_extended_minibatch(x.shape[1],x.shape[0], batch_size=10)\n",
        "my_LR_batches10.train(torch.tensor(x),torch.tensor(y))\n",
        "plt.plot(my_LR_batches10.loss_during_training,'-b',label='$batch size=$'+str(10))\n",
        "print('Weights for '+str(10)+ ' batches:')\n",
        "print(my_LR_batches10.output.weight)\n",
        "print('Bias for '+str(10)+ ' batches:')\n",
        "print(my_LR_batches10.output.bias)\n",
        "\n",
        "\n",
        "my_LR_batches20 = LR_extended_minibatch(x.shape[1],x.shape[0], batch_size=20)\n",
        "my_LR_batches20.train(torch.tensor(x),torch.tensor(y))\n",
        "plt.plot(my_LR_batches20.loss_during_training,'-r',label='$batch size=$'+str(20))\n",
        "print('Weights for '+str(20)+ ' batches:')\n",
        "print(my_LR_batches20.output.weight)\n",
        "print('Bias for '+str(20)+ ' batches:')\n",
        "print(my_LR_batches20.output.bias)\n",
        "\n",
        "my_LR_batches50 = LR_extended_minibatch(x.shape[1],x.shape[0], batch_size=50)\n",
        "my_LR_batches50.train(torch.tensor(x),torch.tensor(y))\n",
        "plt.plot(my_LR_batches50.loss_during_training,'-g',label='$batch size=$'+str(50))\n",
        "print('Weights for '+str(50)+ ' batches:')\n",
        "print(my_LR_batches50.output.weight)\n",
        "print('Bias for '+str(50)+ ' batches:')\n",
        "print(my_LR_batches50.output.bias)\n",
        "\n",
        "my_LR_batches100 = LR_extended_minibatch(x.shape[1],x.shape[0], batch_size=100)\n",
        "my_LR_batches100.train(torch.tensor(x),torch.tensor(y))\n",
        "plt.plot(my_LR_batches100.loss_during_training,'-m',label='$batch size=$'+str(100))\n",
        "print('Weights for '+str(100)+ ' batches:')\n",
        "print(my_LR_batches100.output.weight)\n",
        "print('Bias for '+str(100)+ ' batches:')\n",
        "print(my_LR_batches100.output.bias)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#cuanto mas pequeÃ±o sea el mini-batch, mas gradientes calcula en una sola epoca\n",
        "#por eso cuanto mas pequeÃ±o sea el minibatch, mas rapido baja\n",
        "#y por eso el GD sera el que mas rapido disminuya el error"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training loss after 0.000000e+00 epochs: 0.642481\n",
            "Training loss after 1.000000e+01 epochs: 0.407994\n",
            "Training loss after 2.000000e+01 epochs: 0.302883\n",
            "Training loss after 3.000000e+01 epochs: 0.248033\n",
            "Training loss after 4.000000e+01 epochs: 0.215747\n",
            "Training loss after 5.000000e+01 epochs: 0.186510\n",
            "Training loss after 6.000000e+01 epochs: 0.176118\n",
            "Training loss after 7.000000e+01 epochs: 0.161865\n",
            "Training loss after 8.000000e+01 epochs: 0.151014\n",
            "Training loss after 9.000000e+01 epochs: 0.143633\n",
            "Weights for 10 batches:\n",
            "Parameter containing:\n",
            "tensor([[0.5536, 0.1817, 0.4139, 0.5816, 0.4580, 0.4682, 0.4527, 0.4856, 0.4851]],\n",
            "       requires_grad=True)\n",
            "Bias for 10 batches:\n",
            "Parameter containing:\n",
            "tensor([-0.1578], requires_grad=True)\n",
            "Training loss after 0.000000e+00 epochs: 0.434872\n",
            "Training loss after 1.000000e+01 epochs: 0.357339\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([20, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training loss after 2.000000e+01 epochs: 0.307361\n",
            "Training loss after 3.000000e+01 epochs: 0.272476\n",
            "Training loss after 4.000000e+01 epochs: 0.247134\n",
            "Training loss after 5.000000e+01 epochs: 0.226920\n",
            "Training loss after 6.000000e+01 epochs: 0.211487\n",
            "Training loss after 7.000000e+01 epochs: 0.195174\n",
            "Training loss after 8.000000e+01 epochs: 0.185203\n",
            "Training loss after 9.000000e+01 epochs: 0.178437\n",
            "Weights for 20 batches:\n",
            "Parameter containing:\n",
            "tensor([[0.5367, 0.5558, 0.1252, 0.5299, 0.1909, 0.2358, 0.5177, 0.1864, 0.1837]],\n",
            "       requires_grad=True)\n",
            "Bias for 20 batches:\n",
            "Parameter containing:\n",
            "tensor([-0.2367], requires_grad=True)\n",
            "Training loss after 0.000000e+00 epochs: 0.564546\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([50])) that is different to the input size (torch.Size([50, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training loss after 1.000000e+01 epochs: 0.501995\n",
            "Training loss after 2.000000e+01 epochs: 0.461027\n",
            "Training loss after 3.000000e+01 epochs: 0.423614\n",
            "Training loss after 4.000000e+01 epochs: 0.388900\n",
            "Training loss after 5.000000e+01 epochs: 0.366694\n",
            "Training loss after 6.000000e+01 epochs: 0.340317\n",
            "Training loss after 7.000000e+01 epochs: 0.320250\n",
            "Training loss after 8.000000e+01 epochs: 0.304318\n",
            "Training loss after 9.000000e+01 epochs: 0.288264\n",
            "Weights for 50 batches:\n",
            "Parameter containing:\n",
            "tensor([[-0.0432, -0.0700,  0.4750,  0.4573,  0.1067, -0.0039,  0.4809,  0.2196,\n",
            "          0.2578]], requires_grad=True)\n",
            "Bias for 50 batches:\n",
            "Parameter containing:\n",
            "tensor([-0.3443], requires_grad=True)\n",
            "Training loss after 0.000000e+00 epochs: 0.377873\n",
            "Training loss after 1.000000e+01 epochs: 0.359773\n",
            "Training loss after 2.000000e+01 epochs: 0.357431\n",
            "Training loss after 3.000000e+01 epochs: 0.342824\n",
            "Training loss after 4.000000e+01 epochs: 0.346620\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training loss after 5.000000e+01 epochs: 0.333084\n",
            "Training loss after 6.000000e+01 epochs: 0.323250\n",
            "Training loss after 7.000000e+01 epochs: 0.317750\n",
            "Training loss after 8.000000e+01 epochs: 0.313219\n",
            "Training loss after 9.000000e+01 epochs: 0.304276\n",
            "Weights for 100 batches:\n",
            "Parameter containing:\n",
            "tensor([[0.1939, 0.0890, 0.3579, 0.3080, 0.2403, 0.1562, 0.1880, 0.1661, 0.3385]],\n",
            "       requires_grad=True)\n",
            "Bias for 100 batches:\n",
            "Parameter containing:\n",
            "tensor([0.1253], requires_grad=True)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhRb0Jw5v-l8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "outputId": "00a10704-d1fc-4508-d8f5-51e4724f17bd"
      },
      "source": [
        "my_LR_fullEpoch=LR_extended_minibatch(x.shape[1],x.shape[0],batch_size=1)\n",
        "my_LR_fullEpoch.train(torch.tensor(x),torch.tensor(y))\n",
        "print(my_LR_fullEpoch.loss_during_training)\n",
        "plt.plot(my_LR_fullEpoch.loss_during_training,'-g',label='$Ev with size:$')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training loss after 0.000000e+00 epochs: 0.525811\n",
            "Training loss after 1.000000e+01 epochs: 0.130060\n",
            "Training loss after 2.000000e+01 epochs: 0.106746\n",
            "Training loss after 3.000000e+01 epochs: 0.098573\n",
            "Training loss after 4.000000e+01 epochs: 0.094326\n",
            "Training loss after 5.000000e+01 epochs: 0.091671\n",
            "Training loss after 6.000000e+01 epochs: 0.089832\n",
            "Training loss after 7.000000e+01 epochs: 0.088472\n",
            "Training loss after 8.000000e+01 epochs: 0.087425\n",
            "Training loss after 9.000000e+01 epochs: 0.086595\n",
            "[0.5258105414441514, 0.3374004763796743, 0.25820539027437234, 0.2155157365259602, 0.18911373219786093, 0.17118989355845, 0.1582862947252579, 0.14859133569456082, 0.1410324616393811, 0.13499356239982563, 0.13006021757916655, 0.12594688373551408, 0.12247770115457761, 0.11950801130114624, 0.11693957942192734, 0.11470031654022983, 0.11272624381653015, 0.11097323964706284, 0.1094118283325117, 0.10801107475935613, 0.10674552705758807, 0.1055936447196017, 0.1045431586604752, 0.1035808516833841, 0.10269424155786995, 0.10187729538431758, 0.10112056472666645, 0.10041688490057177, 0.09975919735527901, 0.09914570506442066, 0.09857254872306993, 0.09803312841320874, 0.09752474666409934, 0.09704647647507478, 0.0965945421816238, 0.09616657291128576, 0.09576075055214653, 0.095375072717107, 0.09500870055576613, 0.09465982227265975, 0.09432646457716609, 0.0940079037073108, 0.09370274352840893, 0.09341126761311827, 0.09313204502667813, 0.09286468956378688, 0.09260764069075458, 0.0923603087565068, 0.09212215851754446, 0.09189256565217528, 0.09167145236904278, 0.09145788182598708, 0.09125293218973958, 0.09105414149674684, 0.0908628117600591, 0.09067706784507616, 0.09049718981173807, 0.09032321024016444, 0.0901544814395807, 0.08999092893954053, 0.08983225070177522, 0.08967822616533658, 0.0895284290098476, 0.08938282745566725, 0.0892418611890713, 0.08910470663649651, 0.08897130979208848, 0.08884139382831314, 0.0887149754885574, 0.08859185448791312, 0.0884720239879845, 0.08835518784320037, 0.08824156443934128, 0.08813051081774938, 0.08802250122597247, 0.08791663019377682, 0.0878136731475106, 0.08771286835848167, 0.08761462029144533, 0.08751874403489157, 0.08742485019909654, 0.08733336953438132, 0.08724402870318253, 0.08715607173741545, 0.08707090383088463, 0.08698719311024185, 0.08690509092908785, 0.08682502490530312, 0.0867466407869708, 0.08666998254418833, 0.08659484174022031, 0.08652118229283128, 0.08644879501465695, 0.08637816991119404, 0.08630933683953688, 0.08624165753860338, 0.08617532436718274, 0.08611023403439295, 0.08604658642472318, 0.08598407119631668]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2kkXsxxN9ej"
      },
      "source": [
        "> **(Optional Exercise)**: Try to think how to modify the loss function to incorporate $L_2$ or $L_1$ regularization to the model. Implement a class for regularized logistic regression with a pre-defined regularization parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRCWm8zXfcG2"
      },
      "source": [
        "\n",
        "\n",
        "def loss_with_L2(myLR,lambda_reg='0.01'):\n",
        "  l2 = 0\n",
        "  for w in my_LR.parameters():\n",
        "    if l2==0:\n",
        "      l2= w.norm(2)\n",
        "    else:\n",
        "      l2=l2+w.norm(2)    \n",
        "  loss= (1/N_train)*(y_pred - batch_ys).pow(2).sum() + lambda_reg* l2\n",
        "  loss.backward()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}